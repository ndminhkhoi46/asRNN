{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataModule.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO754Czj/xINjsxLTKLFX6l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pytorch_lightning -q"],"metadata":{"id":"zl-HdKhk5wGy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yv1sxI9b5CkS"},"outputs":[],"source":["#=============================\n","#Import\n","#=============================\n","import numpy as np\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from torch.utils.data import random_split, DataLoader, Dataset\n","import torch\n","from pytorch_lightning import LightningDataModule\n","import os\n","#-----------------------------MNIST Dataset-----------------------------#\n","#=============================\n","#Lightning Data Module Wrapper\n","#=============================\n","class MNISTDataModule(LightningDataModule):\n","  def __init__(self, sequence_length, input_size, batch_size, val_size = 0, permute_seed = None):\n","    super().__init__()\n","    self.sequence_length = sequence_length\n","    self.input_size = input_size\n","    self.batch_size = batch_size\n","    self.val_size = val_size\n","    self.train_size = 60000 - self.val_size\n","    self.root = '/content/drive/My Drive/Colab Notebooks/EXP/Dataset/'\n","    self.idx_permute = None\n","\n","    if permute_seed:\n","      rng_permute = np.random.RandomState(permute_seed)\n","      self.idx_permute = torch.from_numpy(rng_permute.permutation(784))\n","  def setup(self, stage):\n","    if self.idx_permute:\n","      transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1)[self.idx_permute].view(self.sequence_length,self.input_size))])\n","    else:\n","      transform = transforms.ToTensor()\n","\n","    self.mnist_train = MNIST(root=self.root,transform=transform, download=True)\n","    if (self.val_size > 0):\n","      self.mnist_train, self.mnist_valid = random_split(self.mnist_train, [self.train_size, self.val_size])\n","    else:\n","      self.mnist_valid = MNIST(root=self.root,train=False, transform=transform, download=True)\n","    self.mnist_test = MNIST(root=self.root,train=False, transform=transform, download=True)\n","    \n","  def train_dataloader(self):\n","    return DataLoader(self.mnist_train,batch_size=self.batch_size, shuffle = True)\n","  def val_dataloader(self):\n","    return DataLoader(self.mnist_valid,batch_size=self.mnist_valid.__len__())\n","  def test_dataloader(self):\n","    return DataLoader(self.mnist_test,batch_size=self.mnist_test.__len__())\n","\n","#-----------------------------Copy Memory Task Dataset-----------------------------#\n","#=============================\n","#Sample Generator\n","#=============================\n","def generate_copying_sequence(recall_length, delay_length):\n","  marker = [0]*8 + [1]\n","  blank = [0]*9\n","  alphabet = np.array([[0]*i + [1] + [0]*(9-(i+1)) for i in range(8)])\n","  choices_idx = np.random.choice(len(alphabet), size=recall_length, replace=True)\n","  seq_to_be_copied = alphabet[choices_idx,:]\n","  if delay_length > 0:\n","    x = np.vstack((seq_to_be_copied,[blank]*delay_length, [marker], [blank]*(recall_length-1)))\n","  else:\n","    x = np.vstack((seq_to_be_copied, [marker], [blank]*(recall_length-1)))\n","  y = np.vstack(([blank]*(recall_length+delay_length),seq_to_be_copied))\n","  return torch.FloatTensor(x), torch.FloatTensor(y)[:,:-1]\n","#=============================\n","#Batch Generator\n","#=============================\n","def create_copying_batch(recall_length, delay_length, batch_size):\n","    x = []\n","    y = []\n","    for i in range(batch_size):\n","        sample_x, sample_y = generate_copying_sequence(recall_length, delay_length)\n","        x.append(sample_x)\n","        y.append(sample_y)\n","\n","    x = torch.stack(x, axis=0)\n","    y = torch.stack(y, axis=0)\n","    return x, y\n","#-----------------------------PTB Dataset-----------------------------#\n","#=============================\n","#Dictionary\n","#=============================\n","class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)\n","#=============================\n","#Tokenization, Word2VecIdx\n","#=============================\n","class PTBCorpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","    def tokenize(self, path):\n","        \"\"\"Tokenizes a text file.\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                tokens += len(words)\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        # Tokenize file content\n","        with open(path, 'r') as f:\n","            ids = torch.LongTensor(tokens)\n","            token = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    ids[token] = self.dictionary.word2idx[word]\n","                    token += 1\n","        return ids\n","    def trim_batch(self, data, bsz):  #trimming and reshapeing to batches\n","        data = data.view(-1)\n","        nbatch = data.size(0) // bsz\n","        data = data[:nbatch * bsz]\n","        data = data.view(bsz, -1)\n","        return data\n","    def get_ith_sequence(self, source, ith, sequence_length): #get ith-sequence for BPTT\n","        return source[:,ith*sequence_length:(ith+1)*sequence_length] #batch_size X sequence_length"]},{"cell_type":"code","source":["x, y = create_copying_batch(1000,0,1)\n","print(x.shape, y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Dfe_4v5ZVXP","executionInfo":{"status":"ok","timestamp":1653466505846,"user_tz":-420,"elapsed":54,"user":{"displayName":"Khôi Nguyễn Duy Minh","userId":"05400918452609541741"}},"outputId":"077b1fce-037a-466c-9950-0662e704dad8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2000, 9]) torch.Size([1, 2000, 8])\n"]}]}]}